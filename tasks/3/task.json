{
  "id": 3,
  "status": "-",
  "title": "New child projects structure",
  "description": "Create a new structure for child projects that stems from this project. This will be done by creating a new repository for each child project. Each child project is linked backed to this projct via git-submodules so that all the child projects are automatically updated whenever this project updates. This project drives the child projects and then the child projects can also be cloned independently if needed and will drive their own implementation work. The first project is `Docker`. We want to be able to run any project (including this + any child project) in a docker environment. It needs to be extremely easy to launch this - ideally a single function call to get it launched and then whatever function call needs to be called inside the running container. As a first example we should be able to launch this project in docker and call `run.py` to launch an agent exactly as defined in `LOCAL_SETUP.md`. There needs to be a step-by-step guide for this that exactly explains it.",
  "features": [
    {
      "id": "3.1",
      "status": "-",
      "title": "Document submodule workflow for child projects",
      "description": "Create clear documentation detailing how the projects/ folder is used with git submodules, including cloning, initialization, adding, updating, and removing child projects, plus common pitfalls.",
      "plan": "Feature: Document submodule workflow for child projects\n\nGoal\n- Provide a clear, authoritative guide for managing child projects as Git submodules in the parent repository, covering onboarding, daily operations, maintenance, CI/CD, auth, governance, versioning, and troubleshooting.\n\nAssumptions\n- Child projects live as Git submodules under the designated directory `projects/`. Adjust names to actual repo conventions during authoring.\n- Git v2.31+ and SSH access (recommended) or HTTPS with PAT are available to developers and CI.\n\nScope\n- Documentation only (primary). Optionally include small helper scripts and hooks to standardize developer operations.\n\nPlan and Steps\n\nPhase 1: Align on conventions and prerequisites\n1) Confirm directory structure and naming: decide the canonical path for submodules (e.g., children/<child-name>/) and naming conventions.\n2) Define submodule configuration policy:\n   - Tracking branch default (e.g., main) via .gitmodules submodule.<path>.branch.\n   - Shallow checkouts (shallow = true) where appropriate.\n   - Prefer SSH URLs (recommended) and provide HTTPS alternative.\n   - Decide on nested submodules policy (allow/avoid).\n3) Define versioning policy:\n   - Pin to tags for releases; pin to branch heads for ongoing development.\n   - Tagging strategy and compatibility expectations.\n4) Define governance and ownership:\n   - How CODEOWNERS applies to submodule paths.\n   - Approval requirements when bumping submodule pointers.\n5) Document required tools and access:\n   - Git version; SSH keys or PAT; org permissions.\n\nDeliverable: A concise \u201cSubmodule policy\u201d section to include at the top of the document.\n\nPhase 2: Author primary documentation (docs/child-projects/submodules.md)\nStructure and content:\n1) Overview and scope\n   - Why submodules are used; pros/cons; when to use.\n   - Repo structure diagram (textual tree) showing parent and children.\n2) Quickstart (fresh clone)\n   - Commands: git clone, git submodule update --init --recursive, optional config: git config submodule.recurse true.\n   - Verifying status: git submodule status.\n3) Daily workflows\n   a) Add a new child project\n      - Command sequence:\n        - git submodule add -b <branch> <ssh-url> children/<name>\n        - git config -f .gitmodules submodule.children/<name>.branch <branch>\n        - Optional shallow: git config -f .gitmodules submodule.children/<name>.shallow true\n        - git add .gitmodules children/<name>\n        - git commit -m \"Add submodule: <name> at <branch>\"\n      - Notes: URL conventions; .gitmodules review; CODEOWNERS update.\n   b) Update a child to latest\n      - Quick path: git submodule update --remote --merge children/<name>\n      - Manual path: cd children/<name>; git fetch; git checkout <branch-or-tag>; cd -; git add children/<name>; git commit -m \"Bump <name> to <ref>\"\n      - Locking to a tag vs tracking a branch.\n   c) Work on a child repo (feature development)\n      - cd children/<name>; git switch -c feat/x; implement; push; open PR in child repo; merge there; then bump pointer in parent with a commit message referencing the child PR.\n   d) Coordinated multi-repo change\n      - Create matching branches in each child and the parent; open cross-referenced PRs; order of merges; then fast-forward parent pointers; include rollback strategy.\n   e) Verify and commit submodule pointer changes\n      - git status shows modified content at path; ensure no local dirty state in submodules; commit pointer updates.\n4) Maintenance workflows\n   a) Remove a submodule\n      - Commands:\n        - git submodule deinit -f children/<name>\n        - rm -rf .git/modules/children/<name>\n        - git rm -f children/<name>\n        - git commit -m \"Remove submodule <name>\"\n        - Verify .gitmodules updated; if needed remove leftover config sections: git config -f .git/config --remove-section submodule.children/<name> || true\n   b) Rename a submodule path\n      - git mv children/<old> children/<new>; update .gitmodules path entry; git submodule sync --recursive; git add .gitmodules; git commit.\n   c) Migrate an existing folder to a submodule (history preserved)\n      - Option A (subtree split):\n        - git subtree split -P children/<name> -b export/<name>\n        - Create new repo; push: git push <new-url> export/<name>:main\n        - Remove folder; add as submodule per step 3a.\n      - Option B (git filter-repo) with equivalent steps if allowed.\n5) CI/CD considerations\n   - GitHub Actions:\n     - Use actions/checkout@v4 with: submodules: recursive, fetch-depth: 0; set ssh-key or token for private submodules.\n     - Example:\n       - uses: actions/checkout@v4\n         with:\n           submodules: recursive\n           fetch-depth: 0\n           ssh-key: ${{ secrets.DEPLOY_KEY }}\n   - GitLab CI:\n     - before_script: git submodule sync --recursive && git submodule update --init --recursive\n     - Ensure GIT_SSH_COMMAND or CI_JOB_TOKEN auth setup.\n   - Caching strategies; pin to tags for release builds.\n6) Authentication for private submodules\n   - Recommended: SSH with org deploy keys or user keys; URL standardization using git config url.ssh://git@github.com/.insteadOf https://github.com/\n   - Alternative: HTTPS with fine-grained PAT; update Actions secrets and CI variable usage.\n7) Versioning and release management\n   - Policy: branch-tracking during development; release pinning to tags.\n   - Release process: tag child repos; update parent to tags; tag parent; changelog aggregation.\n8) Tooling, hooks, and configs\n   - Git configs: submodule.recurse=true; status.submodulesummary=1.\n   - Pre-commit hook to block committing with dirty submodules.\n   - Pre-push hook to block pushing parent with unpushed child branches.\n   - Optional helper scripts (see Phase 3) referenced here.\n9) Troubleshooting and FAQ\n   - Detached HEAD inside submodules and how to switch branch.\n   - Submodule pointer appears modified unexpectedly (explain pointers).\n   - Updating .gitmodules vs .git/config; use git submodule sync.\n   - Dealing with nested submodules.\n   - Cleaning local state: git submodule foreach --recursive git clean -fdx.\n\nPhase 3: Optional supporting assets (if in scope)\n1) scripts/submodules/add_child.sh\n   - Inputs: repo URL, path, branch; configures .gitmodules, shallow if requested.\n2) scripts/submodules/update_all.sh\n   - Loops over submodules; runs git submodule update --remote --merge; opens a branch and commits pointer bumps.\n3) scripts/submodules/check_clean.sh\n   - Exits non-zero if any submodule has uncommitted changes or unpushed commits.\n4) .githooks/\n   - pre-commit: call check_clean.sh; warn if submodule pointer changed without .gitmodules update when needed.\n   - pre-push: prevent pushes if submodules point to non-origin commits.\n5) Documentation references to these scripts and how to install hooks (core.hooksPath).\n\nPhase 4: Validation and review\n1) Dry-run the documented commands in a throwaway clone to confirm accuracy across macOS/Linux.\n2) Validate CI snippets by triggering a minimal workflow (or using local runner like act where possible).\n3) Obtain review from at least two maintainers focused on Git hygiene and CI.\n\nPhase 5: Adoption\n1) Link the new doc from README and CONTRIBUTING.\n2) Announce in team channels; include a short Quickstart snippet.\n3) Open a tracking issue for future improvements (automation, Renovate support for submodules, etc.).\n\nAcceptance criteria\n- A single, comprehensive document exists at docs/child-projects/submodules.md covering: overview, quickstart, daily workflows (add/update/develop/coordinated), maintenance (remove/rename/migrate), CI/CD, auth, versioning, tooling, troubleshooting.\n- All command sequences are tested and correct.\n- CI examples provided for at least GitHub Actions and GitLab CI.\n- Clear policies on branch tracking vs tagged releases and on authentication are documented.\n- Reviewed and approved by 2 maintainers.\n\nOut of scope / Risks\n- Deep automation of cross-repo PRs (may be proposed later).\n- Migration tooling beyond documented steps (use subtree/filter-repo as guidance).\n- Risk: Private submodules in CI require proper key management\u2014documented mitigations provided.\n\nEstimated effort\n- Authoring and testing: 1\u20132 days.\n- Review and polish: 0.5 day.",
      "context": [
        "docs/FILE_ORGANISATION.md",
        "docs/PROJECTS_GUIDE.md"
      ],
      "acceptance": [
        "A Markdown document exists at docs/PROJECTS_GUIDE.md describing how child projects under the projects/ directory are managed as git submodules.",
        "docs/FILE_ORGANISATION.md is updated to reference/include the new docs/PROJECTS_GUIDE.md file.",
        "docs/PROJECTS_GUIDE.md contains an Overview section explaining the purpose of the projects/ folder and that each child project is a git submodule.",
        "docs/PROJECTS_GUIDE.md provides instructions for cloning the repository with submodules using: git clone --recurse-submodules; and for initializing submodules in an existing clone using git submodule init and git submodule update (including the combined git submodule update --init --recursive variant).",
        "docs/PROJECTS_GUIDE.md explains how to add a new child project as a submodule under projects/<name> using git submodule add -b <branch> <url> projects/<name>, and explicitly mentions committing both .gitmodules and the submodule pointer update in the superproject.",
        "docs/PROJECTS_GUIDE.md explains how to update submodules to newer commits using either git submodule update --remote [--recursive] or by entering the submodule and pulling, then committing the pointer bump in the superproject. It includes examples of git submodule foreach for bulk operations.",
        "docs/PROJECTS_GUIDE.md explains how to switch the tracked branch for a submodule via git config -f .gitmodules submodule.projects/<name>.branch <branch> and notes committing .gitmodules.",
        "docs/PROJECTS_GUIDE.md explains how to remove a child project submodule, including the steps: git submodule deinit -f projects/<name>, git rm -f projects/<name>, and removing the .git/modules/projects/<name> directory, and notes that .gitmodules is updated and committed accordingly.",
        "docs/PROJECTS_GUIDE.md contains a Common pitfalls section that explicitly mentions: detached HEAD in submodules; uncommitted changes in submodules; mixing SSH and HTTPS URLs; forgetting to commit .gitmodules; not committing the submodule pointer update; needing git submodule sync when URLs change; and ensuring CI uses --init --recursive.",
        "docs/PROJECTS_GUIDE.md contains a CI/CD notes section showing recommended commands to fetch submodules in automation: git clone --recurse-submodules and/or git submodule update --init --recursive.",
        "docs/PROJECTS_GUIDE.md includes a Troubleshooting section mentioning git submodule sync (preferably with --recursive) and git submodule status usage.",
        "docs/PROJECTS_GUIDE.md includes a Quick reference or cheat sheet that summarizes the most common commands for clone/init, add, update, switch branch, remove, and status.",
        "All examples use paths under projects/ and illustrate projects/<name> for submodules, and the document makes clear that changes inside a submodule should be committed within the submodule and then the superproject pointer updated and committed.",
        "docs/PROJECTS_GUIDE.md references and demonstrates the following commands at least once: git clone --recurse-submodules; git submodule init; git submodule update; git submodule update --init --recursive; git submodule add -b; git submodule update --remote; git submodule foreach; git submodule status; git config -f .gitmodules submodule.projects/<name>.branch <branch>; git submodule deinit -f; git rm -f projects/<name>; rm -rf .git/modules/projects/<name>; git submodule sync.",
        "docs/PROJECTS_GUIDE.md discusses authentication schemes (SSH vs HTTPS) and warns about mixing them across submodules and CI."
      ]
    },
    {
      "id": "3.2",
      "status": "-",
      "title": "Update .gitignore to ignore projects/ directory and contents",
      "description": "Add .gitignore rules so the projects/ folder (which contains child projects as git submodules) and all files within it are ignored by the main repository, while ensuring submodule tracking via .gitmodules and gitlinks remains intact.",
      "plan": "Implementation plan for feature 3.1: Update .gitignore to ignore projects/ directory and contents\n\n1) Preparation\n- Confirm the repository uses a top-level directory named \"projects\" for child projects.\n- Decide whether the repository must keep an empty projects/ directory tracked (e.g., via a .gitkeep). If not required, ignore everything. If required, add an exception for a single placeholder file.\n\n2) Update .gitignore\n- Locate the root .gitignore file. If it does not exist, create it at the repository root.\n- Open .gitignore and add a new section (avoid duplicating existing rules):\n  # Ignore child projects directory\n  /projects/\n- Notes:\n  - Use forward slashes; Git ignores are POSIX-style regardless of OS.\n  - The leading slash scopes the rule to the repository root so only the top-level projects/ is ignored.\n\n3) Optional: keep a placeholder tracked (only if needed)\n- If the team requires the directory to exist in the repo while ignoring its contents, add an exception rule under the above lines:\n  !/projects/.gitkeep\n- Do NOT add this exception unless we also intend to commit a projects/.gitkeep file in a separate task. This feature only adjusts .gitignore.\n\n4) Validate locally\n- Create a temporary file for verification: echo \"test\" > projects/_ignore_me.txt\n- Run: git status\n  - Expected: projects/_ignore_me.txt should not appear in the status output.\n- If an exception for .gitkeep was added in step 3, create an empty projects/.gitkeep and run: git add projects/.gitkeep\n  - Expected: .gitkeep is tracked while all other files under projects/ remain ignored.\n- Optionally run: git check-ignore -v projects/_ignore_me.txt to confirm the rule and source line.\n\n5) Edge cases and conflicts\n- Ensure there are no un-ignore rules (e.g., !/projects/*) elsewhere in .gitignore that would counteract this.\n- If prior rules existed for projects/, consolidate into a single clear rule to avoid confusion.\n\n6) Documentation\n- Update developer docs/README (in a separate docs task if necessary) to clarify that all child projects live under projects/ and are ignored by default.\n\n7) Acceptance criteria\n- .gitignore contains an entry that ignores the top-level projects/ directory and all its contents.\n- Creating files under projects/ does not show them in git status.\n- No unintended files outside projects/ are affected.\n\n8) Rollback plan\n- If needed, remove the /projects/ rule from .gitignore and commit the change.\n- Run git rm -r --cached projects/ to re-track the directory and its contents if previously ignored.",
      "context": [
        ".gitignore",
        ".gitmodules",
        "docs/FILE_ORGANISATION.md"
      ],
      "acceptance": [
        ".gitignore exists at the repository root.",
        ".gitignore includes an ignore rule that ignores the entire projects/ directory and its contents. Accepted patterns (exact lines): 'projects/', '/projects/', 'projects/**', or '/projects/**'.",
        ".gitignore explicitly unignores the .gitmodules file to keep submodule tracking intact. Accepted patterns (exact lines): '!.gitmodules' or '!/.gitmodules'.",
        ".gitignore does not contain a rule that directly ignores .gitmodules (e.g., '.gitmodules' or '/.gitmodules')."
      ]
    },
    {
      "id": "3.3",
      "status": "-",
      "title": "Script: Child projects structure generation",
      "description": "The script generates the new child projects structure (entities, relationships, and indexes) based on the agreed schema and configuration outlined in feature 3.1. This script must be idempotent, support dry-run mode, and provide comprehensive logging/metrics.",
      "plan": "Objective\n- Implement a robust, idempotent script that generates and maintains the new child projects database structure (entities, relationships, indexes) in accordance with the schema and configuration defined in Feature 3.1.\n- Provide dry-run mode, comprehensive logging, and operational metrics.\n\nAssumptions and Inputs\n- The authoritative schema is defined by Feature 3.1 via a machine-readable configuration (YAML/JSON) and optionally a JSON Schema spec.\n- Primary target database: PostgreSQL (>= 12). Plan will abstract to allow future support for other RDBMS.\n- The script will not hard-code entities but compile them from the config (entities, columns, relationships, indexes). Minimal code changes needed when schema evolves.\n\nHigh-Level Design\n- CLI tool that:\n  1) Loads and validates schema/config (Feature 3.1).\n  2) Introspects the target database state.\n  3) Computes a diff plan of DDL actions required.\n  4) Executes actions in a safe order with idempotency controls.\n  5) Supports dry-run (no changes, outputs plan).\n  6) Emits structured logs and metrics.\n\nCore Capabilities\n- Idempotency\n  - Maintain a schema state table (e.g., infra.child_project_schema_state) storing:\n    - applied_at timestamp\n    - version/source hash of the config (content hash)\n    - tool version\n    - db version\n    - feature version (3.1) and schema version if provided\n  - Introspect live DB (information_schema + pg_catalog) to compare actual vs desired.\n  - Generate only missing/alter statements. Adding tables/columns is safe by default. Destructive changes (drop/alter incompatible) require explicit flags.\n  - Use advisory lock to prevent concurrent executions.\n\n- Dry-run\n  - Compute the full change plan and print in a readable, ordered list with DDL snippets and a summary of impact (counts, index builds, constraints to add, potential locks).\n  - Emit exit code 0 with no changes applied.\n\n- Logging\n  - Structured JSON logging with correlation id, run id, stage, action, duration, status, errors.\n  - Verbosity flags: --quiet, --verbose, --debug.\n\n- Metrics\n  - Expose counters, timers, and gauges:\n    - actions_total by type (create_table, alter_table, create_index, add_fk, etc.)\n    - actions_succeeded_total, actions_failed_total\n    - run_duration_seconds\n    - dry_run flag\n    - lock_wait_seconds\n    - db_roundtrips\n  - Pluggable sinks: stdout JSON; optional OpenTelemetry exporters if env configured.\n\n- Safety\n  - All non-concurrent DDL executed in transactions grouped by dependency batch (e.g., tables before FKs, then indexes).\n  - FK constraints created DEFERRABLE INITIALLY DEFERRED (configurable) to avoid immediate validation locks when backfilling; can run VALIDATE after.\n  - CREATE INDEX CONCURRENTLY for large tables where feasible (if using PostgreSQL, note it cannot be inside a transaction block; the executor will split phases accordingly).\n  - By default, disallow destructive changes (drop table/column, incompatible type). Require --allow-destructive with explicit confirmation or --force-noninteractive.\n\nWork Breakdown Structure\n1) Project scaffolding\n- Create package structure:\n  - src/childprojgen/__init__.py\n  - src/childprojgen/cli.py\n  - src/childprojgen/main.py\n  - src/childprojgen/config_loader.py\n  - src/childprojgen/schema_model.py\n  - src/childprojgen/validator.py\n  - src/childprojgen/db/introspect.py\n  - src/childprojgen/db/executor.py\n  - src/childprojgen/db/dialect_postgres.py\n  - src/childprojgen/planner/differ.py\n  - src/childprojgen/planner/ddl_generator.py\n  - src/childprojgen/state_store.py\n  - src/childprojgen/logging.py\n  - src/childprojgen/metrics.py\n  - src/childprojgen/utils.py\n- Tests:\n  - tests/unit/... for each module\n  - tests/integration/test_apply_structure.py\n- Ops:\n  - examples/config/child_projects.schema.yaml\n  - examples/config/child_projects.schema.json (JSON Schema)\n  - README.md (usage, safety, runbook)\n\n2) Configuration format (driven by Feature 3.1)\n- Expected config (YAML/JSON):\n  - version: semver of schema\n  - entities: list of tables\n    - name, schema\n    - columns: name, type, nullable, default, primary_key, unique, check, references, index hints\n    - options: partitioning, tablespace, comment\n  - relationships: list of FKs\n    - from: schema.table(column[s])\n    - to: schema.table(column[s])\n    - on_update, on_delete, deferrable, initially_deferred\n  - indexes: list of secondary indexes\n    - name, schema.table, columns/expressions, unique, method, where, include, concurrently\n  - enums/domains/types (if used by 3.1)\n  - seeds/metadata (optional)\n- Provide a JSON Schema to validate the config structure and constraints (e.g., mutual exclusivity, required fields).\n\n3) Schema model and validation\n- Implement schema_model.py dataclasses:\n  - DatabaseSchema(version, entities[], relationships[], indexes[], enums[])\n  - Entity(name, schema, columns[], options)\n  - Column(name, db_type, nullable, default, pk, unique, check, references)\n  - Relationship(from_table, from_cols[], to_table, to_cols[], on_update, on_delete, deferrable, initially_deferred)\n  - Index(name, table, columns/expressions, unique, method, where, include, concurrently)\n- validator.py:\n  - Validate against JSON Schema (fastjsonschema or jsonschema)\n  - Custom validations: unique names, FK columns exist, matching types, index cols exist, no duplicate indexes, naming conventions if enforced by 3.1.\n\n4) DB introspection (PostgreSQL v1)\n- db/introspect.py:\n  - Fetch existing schemas, tables, columns (name, type, nullability, defaults), PKs, unique constraints, checks.\n  - Existing FKs (deferrable, actions), existing indexes (unique, expressions, methods, partial/where, include).\n  - Existing enum types and values if used.\n  - Return a CurrentDatabaseState model for differ.\n- Handle casing/quoting consistently.\n\n5) Diff planner\n- planner/differ.py:\n  - Inputs: Desired DatabaseSchema, CurrentDatabaseState\n  - Output: ordered list of Change objects:\n    - CreateSchema, CreateType, CreateTable, AddColumn, AlterColumnTypeSafe, SetNotNull, DropNotNull, AddDefault, DropDefault, AddPrimaryKey, AddUnique, AddCheck, AddForeignKey, CreateIndex, ValidateConstraint, CommentOn, etc.\n  - Rules:\n    - Order: schemas/types -> tables -> columns -> constraints -> indexes -> validations/comments\n    - Prefer additive changes; record destructive deltas separately and only include if allowed.\n    - For incompatible type changes, mark as requires_manual_migration unless --allow-destructive.\n    - FK constraints initially NOT VALID (if configured), then VALIDATE later to minimize lock.\n    - Indexes with concurrently flagged are split into a separate non-transactional phase.\n\n6) DDL generator\n- planner/ddl_generator.py:\n  - Map Change objects to Postgres DDL strings with parameters.\n  - Support reversible/preview-friendly rendering and redaction of literals as needed.\n\n7) Executor and transactions\n- db/executor.py:\n  - Acquire advisory lock (pg_try_advisory_lock) to prevent concurrent runs.\n  - Phases:\n    1) transactional phase A: schemas/types/tables/columns/constraints (NOT VALID FKs)\n    2) non-transactional phase: CREATE INDEX CONCURRENTLY\n    3) transactional phase B: VALIDATE CONSTRAINT for FKs; any comments\n  - Respect dry-run: render and log plan only; no execution.\n  - Respect --allow-destructive: include destructive DDL; else skip and report.\n  - Robust error handling with rollback for transactional phases; resume safe on next run.\n\n8) State store and idempotency\n- state_store.py:\n  - Ensure metadata table exists (create if missing) with minimal lock.\n  - Compute config_hash (e.g., SHA256 of canonicalized config content).\n  - Compare with last applied hash; if equal, exit with no-op; if different, proceed.\n  - Persist run record: run_id, config_hash, tool_version, started_at, finished_at, result, changes_applied.\n\n9) CLI and UX\n- cli.py arguments:\n  - --config PATH (required)\n  - --dsn or separate DB params; also support DATABASE_URL env\n  - --schema-version or infer from config\n  - --dry-run\n  - --allow-destructive (bool)\n  - --force-noninteractive (skip prompts)\n  - --concurrent-indexes [auto|always|never]\n  - --log-level [info|debug|warn]\n  - --metrics [stdout|otlp]; --otlp-endpoint if needed\n  - --timeout, --lock-timeout, --statement-timeout\n  - --advisory-lock-key override\n- Interactive confirmation when destructive changes detected unless forced.\n\n10) Logging and metrics\n- logging.py: JSON logger with run_id correlation and per-action spans.\n- metrics.py: counters and timers; optional OTLP export.\n- Emit summary at end: total changes, duration, any skipped/destructive, next steps.\n\n11) Testing\n- Unit tests\n  - Config loader and validator: valid/invalid configs\n  - Differ for common scenarios: add table, add column, add FK, add index, type widening, destructive attempts\n  - DDL generation snapshots\n- Integration tests (Postgres container)\n  - Fresh DB apply -> no-op on second run\n  - Dry-run prints correct plan\n  - Additive change apply\n  - Concurrent index behavior\n  - Destructive change blocked without flag\n  - Metadata table state and hash behavior\n- Property tests for idempotency (apply twice -> no changes)\n\n12) Performance and scale\n- Batch queries for introspection to minimize round trips.\n- Statement timeouts and retries for metadata operations.\n- Use CREATE INDEX CONCURRENTLY when large tables flagged; fall back with clear messaging.\n\n13) Security and secrets\n- DB credentials via environment or standard secret manager; avoid logging secrets.\n- Parameterize DSN; redact in logs.\n\n14) Documentation and runbook\n- README includes:\n  - Purpose and scope\n  - Configuration reference (inline with Feature 3.1)\n  - Examples and dry-run usage\n  - Safety and destructive changes\n  - Metrics and logging\n  - Troubleshooting (locks, timeouts)\n- CHANGELOG entries.\n\n15) Deliverables\n- Source code under src/childprojgen/\n- Tests with CI workflow\n- Example configs and JSON Schema\n- README and runbook\n\n16) Implementation sequence (step-by-step)\n1. Scaffold project, logging, metrics, CLI skeleton.\n2. Implement config loader and JSON Schema validation; include example config matching Feature 3.1 fields.\n3. Implement schema models and custom validations.\n4. Implement Postgres introspection module.\n5. Implement differ to compute change plan for additive changes; unit test extensively.\n6. Implement DDL generator for each change type.\n7. Implement executor with transactional phases and advisory lock.\n8. Implement state store (metadata table, hash, run records).\n9. Wire dry-run path; format readable plan output.\n10. Implement safety flags for destructive changes and prompt behavior.\n11. Add metrics emission and enrich logs with timings.\n12. Integration tests with a real Postgres instance; refine index/FK behaviors.\n13. Documentation and examples; finalize README.\n14. CI setup and linting/type checks.\n\n17) Non-functional acceptance criteria\n- Idempotent: two consecutive applies on same config produce no changes; exit code 0.\n- Dry-run correctness: shows full plan; does not modify DB.\n- Safety: destructive operations blocked unless explicitly allowed.\n- Observability: logs and metrics provide clear visibility of planned/executed changes and timing.\n- Error handling: clear messages, rollbacks on failure, no partial state.\n\nNotes\n- All actual entity/relationship specifics are sourced from Feature 3.1 config at runtime; this script keeps logic generic to those definitions.\n- Future work: extend to additional dialects, data migration hooks, and online backfills.",
      "context": [
        "docs/FILE_ORGANISATION.md",
        "docs/features/3-child-projects/3.1-schema-and-configuration.md",
        "configs/child_projects/schema.yaml",
        "configs/child_projects/config.yaml",
        "src/db/connection.py",
        "src/db/client.py",
        "src/utils/logging.py",
        "src/utils/metrics.py",
        "src/cli/__init__.py",
        "src/cli/common.py"
      ],
      "acceptance": [
        "CLI interface: The script is executable as `python -m child_projects_structure` and accepts at minimum the following flags: --config <path to JSON or YAML config>, --backend <backend name>, --dsn <connection string or file path>, --dry-run (no mutations), --metrics-out <path to write JSON metrics>, and --log-level <level>. It returns exit code 0 on success and non-zero on error.",
        "Backend support: The script must support at least the 'sqlite' backend for persistence. For sqlite, the --dsn must point to a file path for the database. Structures must be created in that database.",
        "Schema application: Given a valid configuration describing entities, relationships, and indexes, the script creates the corresponding structures in the target backend. For sqlite, this includes creating tables for entities, a join table for many-to-many relationships, and indexes as defined in the config.",
        "Dry-run mode: When invoked with --dry-run, the script performs validation and planning only; no changes are applied to the backend. The metrics must reflect planned operations with zero applied changes. Exit code must be 0.",
        "Idempotency: Applying the same configuration to the same target more than once must be safe and result in zero new changes on subsequent runs. The second run must complete successfully (exit code 0) and metrics must report zero created/updated items and non-zero skipped counts corresponding to already existing structures.",
        "Metrics: When --metrics-out is provided, the script writes a JSON file containing at least: backend (string), dry_run (boolean), config_hash (SHA-256 hex of the config content), duration_ms (number), and a counts object with entities, relationships, and indexes, each providing at least planned, created, updated, and skipped integer counters. The counters must accurately reflect the operations planned/applied in that execution.",
        "Logging: At log level INFO (default), the script logs a summary of operations performed. At DEBUG level, the script logs planned operations. While tests will primarily validate metrics, the implementation must provide comprehensive logging consistent with these expectations.",
        "Error handling: With an invalid or unreadable config file, the script exits with a non-zero exit code and logs an error. (Test focus remains on the happy path; this criterion guides implementation behavior.)"
      ]
    },
    {
      "id": "3.4",
      "status": "-",
      "title": "Script: Migrate/backfill into child project structure",
      "description": "Create a migration/backfill script to move existing data into the new child projects structure defined in feature 3.1, ensuring data integrity, auditability, and minimal downtime.",
      "plan": "- Target: Move all existing data into the new child projects structure introduced by feature 3.1, with minimal downtime, full auditability, and reversibility.\n- Assumptions:\n  - Database: PostgreSQL >= 12 (adjust if different).\n  - Current model has a projects table and related entities (e.g., tasks, permissions, metadata, activity), and feature 3.1 introduces hierarchical child projects (either projects.parent_id or a new child_projects table). Where unknown, plan uses a parent_id model in projects plus a mapping table.\n  - Application supports feature flags and can be deployed to perform dual-reads/writes.\n  - We can add DB triggers/functions for CDC and create staging tables.\n\nPreconditions and dependencies\n- Dependency: 3.1schema changes introducing the child project structure must be finalized or at least specified (names, constraints). If not yet applied, include DDL here under Prepare phase.\n- Confirm business rules for deriving child projects from existing data: e.g., how to determine which projects become parents vs. children; rules for depth, uniqueness, archived/soft-deleted handling, and permission inheritance.\n\nHigh-level approach\n1) Prepare: Add schema elements, staging, mapping, and CDC while the app continues writing to legacy model; enable dual-write capability in app (feature flag).\n2) Backfill: Populate the new hierarchical structure from existing records in batches; maintain a mapping table old_id -> new_id relationships.\n3) CDC catch-up: Replay changes captured during backfill until lag is near-zero.\n4) Switchover: Cut reads/writes to new structure (dual-write > write-primary-new), with brief read-only or low-traffic window if necessary.\n5) Verify: Perform integrity checks, reconciliation reports, and user-facing smoke tests.\n6) Finalize: Decommission old columns/paths and remove CDC.\n7) Rollback plan: Clear, reversible steps at each phase.\n\nDetailed implementation plan\n\nA) Schema preparation (DDL, idempotent)\n- New/confirmed tables and columns (adjust naming to 3.1 spec):\n  - projects: add nullable parent_id UUID referencing projects(id) DEFERRABLE INITIALLY DEFERRED; add path/ltree (optional for faster hierarchy queries); add hierarchy_version INT default 0; add migrated_at TIMESTAMPTZ NULL.\n  - project_hierarchy_map (staging/mapping):\n    - old_project_id UUID PRIMARY KEY\n    - new_project_id UUID NOT NULL REFERENCES projects(id)\n    - old_parent_project_id UUID NULL\n    - new_parent_project_id UUID NULL REFERENCES projects(id)\n    - status TEXT CHECK (status IN ('pending','migrated','verified','failed')) DEFAULT 'pending'\n    - error TEXT NULL\n    - batch_id UUID NULL\n    - created_at, updated_at TIMESTAMPTZ\n  - cdc_project_changes (staging):\n    - change_id BIGSERIAL PRIMARY KEY\n    - op CHAR(1) CHECK (op IN ('I','U','D'))\n    - project_id UUID NOT NULL\n    - changed_at TIMESTAMPTZ DEFAULT now()\n    - payload JSONB -- snapshot of relevant columns\n    - processed BOOLEAN DEFAULT FALSE\n  - Optionally per-related-entity CDC tables if child structure affects FKs (e.g., tasks, permissions), or use a generic cdc_changes with entity_type column.\n- Indexes:\n  - projects(parent_id), projects(path) if using ltree, project_hierarchy_map(status), cdc_project_changes(processed, changed_at).\n- Constraints & triggers:\n  - FK projects(parent_id) -> projects(id), ON DELETE RESTRICT (or desired behavior per 3.1).\n  - Enforce acyclic hierarchy: trigger function to prevent cycles (using ltree or recursive CTE validation). Initially set as DEFERRABLE to ease backfill; later enforce STRICT mode.\n  - BEFORE INSERT/UPDATE trigger on legacy write tables to write to CDC during backfill.\n- Auditability:\n  - Add migration_runs table:\n    - run_id UUID PK, started_at, finished_at, initiated_by, status, total_rows, migrated_rows, verified_rows, errors_count, config JSONB (batch_size, flags).\n  - Add migration_run_logs(run_id, step, message, at, level).\n\nB) Application readiness\n- Introduce feature flags:\n  - child_projects_read_mode: legacy|shadow|new\n  - child_projects_write_mode: legacy|dual|new\n- Implement dual-write in the service layer for projects, and any dependent entities whose FK semantics change. In dual, legacy remains source of truth, new receives mirror writes via code path (idempotent upsert based on natural key or legacy id).\n- Implement dual-read shadow mode to compare results and log diffs without affecting users.\n- All write paths should be idempotent and resilient; use unique constraints/upserts.\n\nC) Backfill algorithm\n- Batch strategy:\n  - Use keyset pagination on projects ordered by id (or created_at), with configurable batch_size (e.g., 2k-10k).\n  - For each batch, in a transaction:\n    - Determine parent-child relationships per 3.1 rules. Examples:\n      - If certain field indicates grouping (e.g., org_id + product_line), create parent if missing, assign children accordingly.\n      - If projects already have a parent_ref in legacy metadata, map directly.\n    - Insert any synthetic parent projects required by the new structure (if 3.1 requires creating new parent nodes), marking them with origin='migration' and migrated_at.\n    - Update projects.parent_id for child records.\n    - Compute path/ltree if used, e.g., update projects.path = parent.path || text2ltree(project_slug_or_id).\n    - Insert/merge mapping rows into project_hierarchy_map with status migrated.\n  - Commit; log progress to migration_runs.\n- Idempotency:\n  - UPSERT on mapping table (ON CONFLICT DO UPDATE) and safe UPDATE on projects with WHERE parent_id IS DISTINCT FROM :new_parent.\n  - Maintain a batch checkpoint in migration_runs.config (last_processed_id).\n  - On retry, skip already-migrated rows (mapping.status IN ('migrated','verified')).\n- Error handling:\n  - On per-row failure, record in project_hierarchy_map.error and continue. Threshold-based circuit breaker (e.g., stop if > N errors in last M rows).\n\nD) Change Data Capture (CDC) for minimal downtime\n- Create triggers on legacy write paths for projects (and dependent entities if relationships shift):\n  - AFTER INSERT/UPDATE/DELETE trigger writes a compact JSON snapshot into cdc_project_changes with op, project_id, payload.\n- Backfill runner phases:\n  1) Full backfill: as in C.\n  2) Catch-up: repeatedly process cdc_project_changes WHERE processed=false ordered by change_id, apply corresponding change to new structure.\n  3) When lag < threshold (e.g., < 1s or < 100 events), proceed to switchover.\n- CDC application logic:\n  - For I/U: upsert into new structure (update parent_id/path if required).\n  - For D: enforce deletion/archival semantics consistent with 3.1 (likely soft-delete).\n  - Mark processed=true upon success.\n\nE) Verification and reconciliation\n- Automated checks:\n  - Counts: total project rows equal; parented vs. unparented distributions as expected.\n  - Referential integrity: no cycles; every child has valid parent if rule requires; no orphans unless allowed.\n  - Checksums: hash of key columns (name, status, org_id, etc.) between legacy and new representations via joins on mapping table.\n  - Sample deep compare: random sample N projects; fetch via legacy read and via new read; compare normalized JSON.\n  - Permission equivalence: ensure effective permissions for sampled users/projects unchanged.\n- SQL snippets (examples):\n  - Cycle check: WITH RECURSIVE t AS (SELECT id, parent_id, id AS root FROM projects UNION ALL SELECT p.id, p.parent_id, t.root FROM projects p JOIN t ON p.parent_id=t.id) SELECT root FROM t GROUP BY root HAVING COUNT(*) > 10000; -- abnormal depth\n  - Orphans: SELECT id FROM projects WHERE parent_id IS NOT NULL AND parent_id NOT IN (SELECT id FROM projects);\n  - Count parity: SELECT (SELECT count(*) FROM projects) AS total_new, (SELECT count(*) FROM legacy_projects) AS total_old;\n- Record per-batch verification status in project_hierarchy_map (status=verified) and update migration_runs.\n\nF) Switchover (low-risk sequence)\n- Pre-switchover checklist:\n  - All backfill complete; CDC lag tiny; verification checks passing within tolerance.\n  - Dual-write enabled and stable; error rate negligible.\n- Steps:\n  1) Freeze configuration changes window or briefly set app to read-only for project mutations (if necessary for ultra-low risk).\n  2) Drain CDC: stop app writes for 15-30 seconds or reduce traffic, process remaining CDC queue to zero.\n  3) Flip feature flags: child_projects_write_mode=new, child_projects_read_mode=new.\n  4) Monitor for 10-30 minutes; keep CDC running in shadow mode and reconcile any stragglers if needed.\n  5) Disable legacy writes (remove dual-write) once stable.\n\nG) Finalization\n- Remove CDC triggers and stop CDC consumers after stability period.\n- Tighten constraints:\n  - Make any previously deferrable constraints NOT DEFERRABLE if desired.\n  - Enforce NOT NULL parent_id where required by 3.1 (if all projects must belong to a parent) or keep nullable if top-level parents exist.\n- Drop or deprecate obsolete legacy columns/tables after retention period.\n- Update documentation and data contracts for downstream consumers.\n\nH) Rollback strategy\n- Before switchover: Simply stop backfill, drop/disable CDC triggers, and leave app on legacy paths.\n- After switchover but within rollback window:\n  - Flip feature flags back: write_mode=legacy, read_mode=legacy.\n  - Re-enable dual-write to legacy if it was disabled.\n  - Optionally write a reverse-CDC (new->legacy) replayer if needed, or accept a limited data freeze window while reverting.\n  - Because legacy data remained intact, revert is configuration-only with minimal data manipulation.\n\nI) Operational runbook\n- Environments:\n  - Staging full dress rehearsal with production-sized snapshot, measure timings, tune batch_size and indexes.\n- Steps (prod):\n  1) Enable dual-write shadow in app (read_mode=legacy, write_mode=dual).\n  2) Apply schema prep DDL and triggers.\n  3) Start backfill job with batch_size tuned from staging; monitor metrics.\n  4) Start CDC catch-up loop; ensure lag metric.\n  5) Run verification suite; remediate anomalies.\n  6) Switchover per steps in F.\n  7) Observe; if stable, finalize and decommission CDC.\n  8) Post-migration report and sign-off.\n\nJ) Implementation details\n- CLI/Job structure (language per codebase):\n  - cmd: migrate_child_projects\n    - prepare: apply DDL; create triggers; seed migration_runs.\n    - backfill --batch-size=5000 --concurrency=4 --from-id=... --to-id=...\n    - cdc-replay --max-batch=10000 --until-lag-ms=1000\n    - verify --sample=1000 --checks=counts,orphans,permissions\n    - switchover --confirm\n    - finalize --drop-cdc --tighten-constraints\n    - rollback --to=legacy\n  - All commands idempotent; support --dry-run to print planned actions.\n- SQL/PLpgSQL helpers:\n  - upsert_project_hierarchy(old_id, parent_old_id) RETURNS VOID\n  - recompute_path(project_id) RETURNS VOID\n  - cdc trigger function: cdc_project_changes_fn()\n- Concurrency and locking:\n  - Use SKIP LOCKED when scanning mapping rows for worker concurrency.\n  - Keep transactions per batch short; avoid table-wide locks.\n- Performance tuning:\n  - Create temporary indexes on staging tables;\n  - Increase work_mem for path recomputations; disable synchronous_commit for backfill transactions if acceptable.\n  - Vacuum/analyze affected tables post-migration.\n\nK) Monitoring and alerting\n- Metrics:\n  - backfill.rows_processed, backfill.rows_failed, backfill.throughput\n  - cdc.queue_depth, cdc.apply_latency_ms\n  - verification.diff_rate, orphan_count, cycle_detected\n  - app.errors.child_project_write, read_diff_rate (shadow mode)\n- Dashboards and alerts for thresholds (e.g., cdc.apply_latency_ms > 5000, errors > 0.1%).\n\nL) Testing strategy\n- Unit tests:\n  - Mapping logic from legacy attributes to child relationships.\n  - Cycle prevention trigger behavior.\n  - CDC trigger correctness and payload completeness.\n- Integration tests:\n  - End-to-end backfill with a miniature dataset, including edge cases: archived projects, soft-deleted, deep nesting, circular refs, missing parents.\n  - Dual-write/read consistency tests.\n- Load tests:\n  - Backfill performance on large synthetic datasets; verify no lock contention.\n\nM) Data considerations and edge cases\n- Soft-deleted/archived: Preserve state; do not resurrect; maintain parent_id but avoid creating parents for fully archived sets unless required.\n- Orphans: Define policy: top-level parents allowed (parent_id NULL) or must attach to synthetic parent per org/tenant.\n- Circular references in legacy: Detect and break using deterministic rule (e.g., lowest id becomes parent).\n- Permissions: If effective permissions depend on hierarchy, compute equivalence and migrate role bindings.\n- External integrations: Communicate schema changes; keep compatibility views if necessary.\n\nN) Timeline and responsibilities (indicative)\n- Week 1: Finalize mapping rules; implement DDL, triggers, CLI skeleton; unit tests.\n- Week 2: Implement backfill + CDC; staging rehearsal; performance tuning.\n- Week 3: Production backfill; verification; switchover; finalize; post-mortem.\n\nDeliverables\n- Migration CLI/job with documented flags.\n- SQL migrations (up/down) for schema prep and rollback.\n- Runbook and monitoring dashboards.\n- Test suite and staging rehearsal report.\n",
      "context": [
        "docs/FILE_ORGANISATION.md",
        "tasks/3/task.json",
        "tasks/3/tests/test_3_1.py",
        "tasks/3/tests/test_3_4.py"
      ],
      "acceptance": [
        "A Python migration/backfill script exists at module path 'scripts.migrate_child_projects' exposing a callable 'migrate(fetcher, writer, auditor, batch_size=1000, dry_run=False, resume_from=None, logger=None)'.",
        "The migrate function consumes legacy project records from 'fetcher(resume_from, batch_size)' which yields batches (lists) of dicts with keys: 'id' (int) and 'legacy_parent_id' (int or None).",
        "For each record where 'legacy_parent_id' is not None, the script writes a single parent->child relation into the new child projects structure via 'writer.create_relation(parent_id, child_id)'.",
        "Before creating a relation, the script verifies both the parent and child exist using 'writer.exists_project(project_id)' and skips with error logging if either is missing.",
        "The script prevents invalid relations: self-parenting (parent_id == child_id) must be skipped and counted as failures; such cases must log an error.",
        "Idempotency: if 'writer.relation_exists(parent_id, child_id)' returns True, the script must not create a duplicate relation. A second run on the same data creates no duplicates and reports the correct number of skipped existing relations.",
        "Dry-run mode: when 'dry_run=True', no calls to 'writer.create_relation' are made, but processing, logging, and summary counts still reflect what would be processed (created reported as 0).",
        "Batch processing: the script processes records in batches based on 'batch_size'. If the 'writer' implements optional 'begin_batch()' and 'commit_batch()' methods, the script calls them once per processed batch.",
        "Resume support: the script accepts 'resume_from' (an integer checkpoint). It must pass 'resume_from' through to 'fetcher'. The script returns a 'checkpoint' value equal to the highest 'id' seen during processing, allowing subsequent runs to resume from that point.",
        "Audit logging: for each successfully created relation, the script calls 'auditor.log(\"child_project_backfill\", details)', where details includes at least 'parent_id', 'child_id', and 'source' set to 'legacy'.",
        "Progress and error logging: the script logs batch progress and totals via 'logger.info' and invalid cases via 'logger.error'.",
        "Summary: the migrate function returns a dict with keys: 'processed' (number of candidate records with non-null legacy_parent_id examined), 'created', 'skipped_existing', 'failed', 'dry_run', and 'checkpoint'. It must satisfy: processed == created + skipped_existing + failed.",
        "Minimal downtime expectation: the script's batch processing and resume capability allow for online/rolling execution without full-table locks (validated indirectly via batch and resume behavior in tests)."
      ]
    },
    {
      "id": "3.5",
      "status": "-",
      "title": "Script: Post-migration validation and cleanup",
      "description": "Create a script to validate the new child projects structure post-migration and perform safe cleanup of obsolete/duplicate/orphaned data. Provide comprehensive reporting for sign-off.",
      "plan": "Objective\n- Build a robust, idempotent script to validate the new child projects structure after migration and safely clean up obsolete, duplicate, and orphaned data. Provide comprehensive reporting (HTML/CSV/JSON) suitable for leadership sign-off.\n\n2) Assumptions & Inputs\n- Primary datastore: PostgreSQL (adaptable to other RDBMS with minor SQL adjustments).\n- Migration produced a manifest (JSON/YAML/CSV) mapping old_project_id -> new_child_project_id and expected counts per parent project/organization.\n- Schema (representative, adjust to actual):\n  - projects(id, parent_project_id NULLABLE, name, slug, type [parent|child], org_id, created_at,...)\n  - child_projects is logically represented by projects where type='child' (if separate table exists, adapt queries accordingly).\n  - tasks(id, project_id, ...), comments(id, task_id,...), attachments(id, owner_type, owner_id, storage_path,...)\n  - project_memberships(user_id, project_id, role, ...)\n  - permissions/ACL tables, labels/tags tables, cross-references tables (project_links, project_labels, etc.)\n- Access to object storage (S3 or filesystem) for attachments.\n- Read-only validation must work without cleanup permissions; cleanup requires elevated credentials.\n\n3) High-Level Approach\n- Create a Python CLI tool with two phases: validate and cleanup, defaulting to dry-run. Produce artifacts in a run directory with unique run_id.\n- Validation suite performs structural, referential, duplicate, and consistency checks against the migrated child project structure and dependent entities.\n- Cleanup suite, when enabled, quarantines affected rows, performs safe merges or deletions within transactions, and writes reversible change logs.\n- Reporting compiles a sign-off package: summary, details, diffs, and CSV/JSON exports for audit.\n\n4) Detailed Steps\n4.1) Project scaffolding\n- Language & libs: Python 3.11+, Click/Typer (CLI), SQLAlchemy (DB), psycopg2-binary, Jinja2 (HTML report), pydantic (config), ruamel.yaml (YAML), orjson (JSON), pandas (CSV summaries), structlog/logging.\n- Structure:\n  - scripts/\n    - cli.py (entrypoint)\n    - config.py (config schema)\n    - db.py (engine, sessions, advisory lock)\n    - validation/\n      - structural.py\n      - referential.py\n      - duplicates.py\n      - consistency.py\n      - cross_system.py\n    - cleanup/\n      - quarantine.py\n      - duplicates.py\n      - orphans.py\n      - obsolete.py\n    - reporting/\n      - reporter.py (HTML/CSV/JSON)\n      - templates/\n        - report.html.j2\n    - models.py (dataclasses for results, run metadata)\n    - utils.py (io, batching, time, progress)\n  - tests/ (unit + integration)\n  - runbooks/ (operator instructions)\n  - config.sample.yaml\n\n4.2) Configuration & Inputs\n- config.sample.yaml keys:\n  - database:\n    - dsn: env or URI\n    - schema: optional\n  - storage:\n    - type: s3|fs\n    - s3_bucket / base_path\n  - manifests:\n    - migration_mapping: path to JSON/CSV mapping (old->new)\n    - expected_counts: path to expected totals (by org/parent)\n  - execution:\n    - dry_run: true (default)\n    - batch_size: 5_000\n    - max_runtime_minutes: 240\n    - advisory_lock_key: 9374501\n  - reporting:\n    - out_dir: ./runs\n    - include_details: true\n  - retention:\n    - quarantine_days: 30\n- Load manifests; validate against schema. Fail early if missing.\n\n4.3) Connectivity & Safety Checks\n- Verify DB connectivity, user privileges detection (read-only vs maintenance).\n- Acquire single-run advisory lock (Postgres pg_try_advisory_lock). Exit if not acquired.\n- Ensure backup/snapshot was taken: record confirmation flag or auto-check latest PITR snapshot marker if available.\n- Start run metadata record in table migration_validation_runs(run_id, started_at, user, dry_run, git_sha, config_hash,...)\n\n4.4) Validation Suite \u2013 Checks\n4.4.1) Structural Integrity\n- Verify new child projects exist and type='child'; parents type='parent'.\n- Count checks vs expected:\n  - Total number of child projects per org and per parent matches expected_counts.\n  - No child project without a parent (parent_project_id not null and parent exists).\n- Sample SQL:\n  - Missing parents:\n    SELECT c.id FROM projects c LEFT JOIN projects p ON p.id=c.parent_project_id WHERE c.type='child' AND p.id IS NULL;\n  - Count by parent vs expected (load expected into temp table expected_child_counts(parent_project_id, expected_count)):\n    SELECT p.id, COUNT(c.id) actual, e.expected_count FROM projects p JOIN expected_child_counts e ON e.parent_project_id = p.id LEFT JOIN projects c ON c.parent_project_id=p.id AND c.type='child' GROUP BY p.id,e.expected_count HAVING COUNT(c.id)<>e.expected_count;\n\n4.4.2) Referential Integrity & Orphans\n- Tasks: ensure tasks.project_id points to child projects only (if business rule requires). Find tasks pointing to parents or non-existent projects.\n  - SELECT t.id, t.project_id FROM tasks t LEFT JOIN projects p ON p.id=t.project_id WHERE p.id IS NULL OR p.type <> 'child';\n- Comments: ensure task exists.\n  - SELECT c.id FROM comments c LEFT JOIN tasks t ON t.id=c.task_id WHERE t.id IS NULL;\n- Attachments: ensure owner exists.\n  - SELECT a.id, a.owner_type, a.owner_id FROM attachments a LEFT JOIN tasks t ON (a.owner_type='task' AND a.owner_id=t.id) LEFT JOIN comments c ON (a.owner_type='comment' AND a.owner_id=c.id) WHERE (a.owner_type='task' AND t.id IS NULL) OR (a.owner_type='comment' AND c.id IS NULL);\n- Memberships/ACLs: ensure referenced project exists and deduplicate by unique key (user_id, project_id, role).\n  - Duplicates:\n    SELECT user_id, project_id, role, COUNT(*) FROM project_memberships GROUP BY user_id, project_id, role HAVING COUNT(*)>1;\n\n4.4.3) Duplicate Detection\n- Child project duplicates within same parent by normalized name/slug:\n  - SELECT parent_project_id, LOWER(name) AS norm, COUNT(*) FROM projects WHERE type='child' GROUP BY parent_project_id, LOWER(name) HAVING COUNT(*)>1;\n  - Same for slug.\n- Labels/tags duplicates per project:\n  - SELECT project_id, LOWER(name), COUNT(*) FROM labels GROUP BY project_id, LOWER(name) HAVING COUNT(*)>1;\n- Project links duplicates by (src_project_id, dst_project_id, type):\n  - SELECT src_project_id, dst_project_id, type, COUNT(*) FROM project_links GROUP BY src_project_id, dst_project_id, type HAVING COUNT(*)>1;\n\n4.4.4) Consistency Rules\n- Slug uniqueness scoped to parent; enforce slug regex.\n- Task counts per project consistent with aggregation columns (if denormalized fields exist), detect drift.\n- Ownership/org consistency: child.org_id equals parent.org_id.\n  - SELECT c.id FROM projects c JOIN projects p ON p.id=c.parent_project_id WHERE c.org_id<>p.org_id;\n- Timestamp sanity: created_at <= updated_at.\n\n4.4.5) Cross-System Validations\n- Search index: sample N projects and verify presence in search index (if accessible); otherwise enqueue reindex tasks and record.\n- Cache invalidation: confirm cache bust markers emitted during migration; if not, record actions.\n- Storage: verify attachments paths exist in storage; list missing objects (guarded and sampled to avoid cost).\n\n4.5) Cleanup Plan (Dry-run vs Apply)\n- Dry-run: default; compute actions only and stage them in report + write to quarantine preview tables without deleting main data.\n- Apply mode: requires --apply --confirm flags.\n- All destructive actions are transactional and reversible using quarantine and logs.\n\n4.5.1) Quarantine/Staging\n- Create tables if not exist:\n  - duplicates_quarantine(run_id, table_name, pk, payload JSONB, reason, created_at)\n  - orphans_quarantine(run_id, table_name, pk, payload JSONB, reason, created_at)\n  - cleanup_actions(run_id, action_id, action_type, target_table, target_pk, details JSONB, status [planned|done|rolled_back], error)\n- Insert candidates during validation pass. In dry-run, stop here.\n\n4.5.2) Duplicate Resolution\n- Project duplicates within same parent:\n  - Determine canonical row: newest by activity or lowest id; configurable strategy.\n  - Merge strategy: re-point foreign keys (tasks, memberships, labels, links) from duplicates to canonical; then soft-delete duplicates.\n  - SQL pattern:\n    - UPDATE tasks SET project_id=:canonical WHERE project_id IN (:dups) AND project_id<>:canonical;\n    - INSERT INTO project_delete_quarantine SELECT run_id, id, to_jsonb(p.*), 'duplicate' FROM projects p WHERE id IN (:dups_except_canonical);\n    - UPDATE projects SET deleted_at=now(), delete_reason='duplicate', deleted_run_id=:run WHERE id IN (:dups_except_canonical);\n- Membership duplicates: keep one, delete extras after moving to quarantine.\n  - DELETE FROM project_memberships pm USING (SELECT user_id, project_id, role, ctid FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY user_id, project_id, role ORDER BY id) rn FROM project_memberships) t WHERE rn>1) d WHERE pm.ctid=d.ctid RETURNING pm.* -> quarantine.\n- Labels/links duplicates: same pattern.\n\n4.5.3) Orphans\n- Tasks pointing to parent/non-existent projects:\n  - If mapping exists: remap using migration_manifest.\n  - Else: quarantine task rows and soft-delete (configurable).\n- Comments without tasks: quarantine then delete.\n- Attachments with missing owners: if storage object exists, quarantine DB row and consider reassign; otherwise quarantine then delete row and optionally move object to a dead-letter folder in storage.\n\n4.5.4) Obsolete/Legacy Artifacts\n- Legacy mapping tables from pre-migration:\n  - Archive to legacy_schema or export to CSV, then drop or mark as deprecated after retention period.\n- Old columns (e.g., legacy_project_id): ensure fully backfilled and unused, then drop in a separate DDL migration after sign-off.\n\n4.5.5) Post-Cleanup Normalization\n- Recompute denormalized counters (if present) for affected projects.\n- Rebuild relevant indexes (VACUUM/ANALYZE) if necessary.\n- Emit reindex/search jobs for changed entities.\n\n4.6) Reporting & Sign-off Package\n- Outputs per run_id in out_dir/run_<timestamp>_<run_id>/\n  - summary.json (counts, timings, pass/fail per check)\n  - issues/*.csv (duplicates.csv, orphans.csv, referential.csv, storage_missing.csv)\n  - actions_planned.csv and actions_executed.csv\n  - report.html (executive summary + technical detail)\n  - logs.txt\n- HTML report contents:\n  - Overview: run metadata, environment, config hash\n  - Key metrics: total projects, children, parents, tasks remapped, rows quarantined, rows deleted (if apply)\n  - Check results: pass/fail with counts and sample rows\n  - Cleanup results: before/after diffs, top offenders\n  - Storage audit summary\n  - Risk & residuals\n  - Sign-off checklist with placeholders for approvers (Data Eng, App Owner, QA, Security)\n\n4.7) Idempotency, Audit, Rollback\n- Each action recorded in cleanup_actions with run_id. Quarantine retains full row payloads for recovery.\n- Rollback subcommand: reads cleanup_actions for a given run_id in reverse order and restores from quarantine tables.\n- Guardrails: do not re-run apply for same run_id; create new run_id; advisory lock prevents concurrent conflicting runs.\n\n4.8) Performance & Scalability\n- Batch processing for large tables (LIMIT/OFFSET or keyset pagination by id).\n- Use indexes: ensure indexes on FK columns used in joins (project_id, parent_project_id). Optionally create temporary indexes if missing and authorized.\n- Avoid long-lived locks: chunk updates within transactions per batch; commit per batch.\n- Progress reporting every N rows; ETA estimates.\n\n4.9) Security & Access\n- Read-only mode possible for validation.\n- Apply mode requires dedicated service account with limited DML; DDL changes excluded here.\n- Secrets via environment or secret manager; no logs of DSN or PII.\n\n4.10) Testing Strategy\n- Unit tests for each validator to generate expected issue sets from fixtures.\n- Integration tests using ephemeral Postgres (docker) with seed data simulating edge cases: missing parents, duplicates, orphans, mixed orgs.\n- Cleanup tests verifying quarantine population, referential remap, and rollback restores original state.\n- Snapshot tests for HTML report structure.\n\n4.11) Runbook (Operator Instructions)\n- Pre-req: confirm latest DB snapshot taken; ensure manifests present.\n- Dry-run validation:\n  - python -m scripts.cli validate --config config.yaml\n- Review report in out_dir; triage counts; adjust thresholds if needed.\n- Apply cleanup:\n  - python -m scripts.cli cleanup --config config.yaml --apply --confirm\n- Post-apply validate again and produce final sign-off report.\n- If issues, run rollback for specific run_id.\n\n4.12) Acceptance Criteria\n- 100% of structural and referential checks pass post-cleanup.\n- No tasks/comments point to parent or missing projects.\n- Zero duplicate projects within the same parent by name/slug.\n- All actions logged with run_id; rollback verified in tests.\n- Reports generated with accurate counts and sample details.\n- Dry-run safe by default; apply requires explicit confirmation.\n\n5) Key SQL/Pseudocode Snippets\n- Advisory lock acquire:\n  SELECT pg_try_advisory_lock(:lock_key) AS acquired;\n- Run record:\n  INSERT INTO migration_validation_runs(run_id, started_at, dry_run, config_hash) VALUES (:run_id, now(), :dry_run, :hash);\n- Quarantine insert (generic):\n  INSERT INTO duplicates_quarantine(run_id, table_name, pk, payload, reason) SELECT :run_id, 'projects', p.id, to_jsonb(p.*), 'duplicate_name' FROM projects p WHERE ...;\n- Remap tasks to canonical project:\n  UPDATE tasks SET project_id=:canonical WHERE project_id IN (:dups) AND project_id<>:canonical;\n- Soft-delete duplicate projects:\n  UPDATE projects SET deleted_at=now(), delete_reason='duplicate', deleted_run_id=:run WHERE id IN (:dups_except_canonical);\n\n6) Code Outline (CLI)\n- Commands:\n  - validate: runs all validators, populates quarantine tables in dry-run mode (preview), writes report.\n  - cleanup: executes planned actions from validators or recomputes then applies, respecting --apply.\n  - rollback --run-id: reverses actions recorded in cleanup_actions using quarantine snapshots.\n  - report-only --run-id: regenerates report from stored artifacts.\n\n7) Timeline & Estimates\n- Day 1-2: Scaffolding, config, DB utilities, advisory lock, run registry.\n- Day 3-5: Implement validators (structural, referential, duplicates, consistency) + unit tests.\n- Day 6-7: Cleanup flows (quarantine, duplicates, orphans) + rollback; integration tests.\n- Day 8: Reporting (HTML/CSV/JSON) + sample templates.\n- Day 9: Runbook, acceptance tests on staging data, performance tuning.\n- Day 10: Buffer for refinements and sign-off preparation.\n\n8) Risks & Mitigations\n- Long-running locks: mitigate with batch updates and shorter transactions.\n- False positives in duplicates: configurable canonical selection and preview in report; operator override list.\n- Missing manifests: fail fast and allow manual mapping override.\n- Storage checks cost: sample rate configurable; full audit optional.\n\n9) Deliverables\n- Python CLI codebase with tests.\n- config.sample.yaml and schema docs for manifests.\n- SQL migration for support tables (quarantine, runs, actions).\n- Runbook and operator guide.\n- Sample sign-off report with synthetic data.\n",
      "context": [
        "docs/FILE_ORGANISATION.md",
        "scripts/task_utils.py",
        "tasks/3/task.json",
        "tasks/3/tests/test_3_5.py"
      ],
      "acceptance": [
        "Validation: The script verifies referential integrity between parent projects and child projects. After cleanup, no orphan child projects remain (every child.parent_id references an existing parent).",
        "Uniqueness: For each parent, child project codes are unique. Duplicate child projects (e.g., same parent_id and code or same legacy_id) are detected and remediated by consolidating to a single surviving record according to a deterministic rule (e.g., lowest id or earliest created_at retained). All consolidations are listed in the report.",
        "Counts reconciliation: When an expected counts mapping is provided (expected child count per parent), deviations are reported. After cleanup, actual counts match expected counts for all parents; any remaining mismatches are flagged as errors in the report.",
        "Dry-run safety: By default (no --apply), the script performs no data mutations. The report includes planned actions (what would be changed) and executed actions is empty. The dataset remains unchanged.",
        "Apply mode transactional safety: With --apply, data modifications (e.g., removing duplicates, removing/archiving orphans) are executed within a transaction. On any error, the transaction is rolled back and no partial changes persist. The report clearly indicates rollback and the exit code is non-zero.",
        "Idempotency: After a successful apply run on a dataset, re-running the script (apply or dry-run) yields zero planned and zero executed actions, and exit code 0.",
        "Reporting artifacts: The script produces comprehensive reports in both machine-readable JSON and human-readable Markdown in a specified directory. The JSON report includes at minimum: timestamp, summary metrics (total_parents, total_children, errors_count, warnings_count), issues breakdown (orphans, duplicates, count mismatches), planned_actions, executed_actions, and status. The report also references a log file path.",
        "Audit logging: A detailed log file is generated per run (timestamped) that records validations performed and actions taken (or planned) with stable identifiers. The report contains the absolute path to this log.",
        "Configuration and policies: The script accepts configuration via CLI flags and/or a config file, including: fail_on severity (errors|warnings|never), duplicate remediation policy (archive or delete duplicates), and report output directory. Missing required confirmation for destructive actions must prevent execution unless explicitly confirmed.",
        "Exit codes: The script exits with code 0 when no errors are present post-validation/cleanup; 1 when errors remain or a transactional failure occurs; and 2 when only warnings are present (unless fail_on is set to 'never', in which case 0 is returned even if issues are detected)."
      ]
    }
  ]
}